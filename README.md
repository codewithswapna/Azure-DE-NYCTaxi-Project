# ðŸš– NYC Taxi â€” Azure Data Engineering Project  

## ðŸ‘¤ Author: Kamsali Swapna  

## ðŸ“Œ Introduction  
In the era of big data, efficient data engineering pipelines are the backbone of insightful analytics. This project presents an end-to-end **Azure Data Engineering solution** for NYC taxi data, leveraging **Azure Data Factory, Databricks, Data Lake, PySpark, and Delta Lake**.  

### ðŸ”¹ Key Highlights:  
âœ” Scalable data pipeline using **Medallion Architecture** (Bronze, Silver, Gold)  
âœ” Optimized storage & analytics using **Delta Lake**  
âœ” End-to-end orchestration via **Azure Data Factory**  
âœ” **Power BI** integration for real-time reporting  

---

## ðŸŽ¯ Learning Objectives  
âœ… Understand real-time **data integration** in Azure Data Factory  
âœ… Gain hands-on experience with **Databricks & PySpark** for data processing  
âœ… Implement **Delta Tables (Delta Lake)** for optimized storage  
âœ… Apply **Medallion Architecture** for structured data workflows  
âœ… Prepare for **real-world Data Engineering interview questions**  

---

## ðŸ› ï¸ Technologies Used  
| Technology       | Purpose |  
|-----------------|--------------------------------|  
| **Azure Data Factory** | Orchestrating data pipelines |  
| **Azure Data Lake** | Scalable and secure data storage |  
| **Azure Databricks** | Big data processing & analytics |  
| **PySpark** | Distributed data processing |  
| **Delta Lake** | ACID transactions & optimized storage |  
| **Power BI** | Data visualization & reporting |  

---

## ðŸ“Œ Problem Statement  
The NYC taxi dataset contains **massive volumes of raw trip records** that require cleaning and processing for meaningful insights. Traditional ETL workflows face challenges like **scalability, data consistency, and real-time processing**.  

**Solution:**  
âœ… Build a **scalable & efficient** data pipeline using **Azure Data Factory, Databricks, PySpark, and Delta Lake**.  
âœ… Implement **Medallion Architecture** (Bronze, Silver, Gold) for structured processing.  
âœ… Enable **fast processing & reliable storage** for **reporting & machine learning**.  

---

## ðŸ“Š Project Architecture  

### ðŸ—ï¸ End-to-End Data Pipeline: Ingestion, Transformation & Reporting  

#### **ðŸ”¹ Ingestion Layer**  
ðŸ“Œ **Source**: NYC Taxi API (trip records)  
ðŸ“Œ **Azure Data Factory**: Ingests raw data into **Azure Data Lake Gen2**  

#### **ðŸ”¹ Transformation Layer (Databricks & PySpark)**  
- **Bronze Layer (Raw Data)**: Stored in **Parquet format**  
- **Silver Layer (Transformed Data)**: Cleaned & structured  
- **Gold Layer (Serving Data)**: Optimized using **Delta Lake**  

#### **ðŸ”¹ Security**  
ðŸ” **Azure Key Vault** manages authentication & data access  

#### **ðŸ”¹ Reporting Layer**  
ðŸ“Š **Power BI**: Visualizing insights  

---

## ðŸš€ Methodologies  

### **Phase 1: Data Ingestion (Bronze Layer)**  
ðŸ“¥ **Raw data ingestion** from the **NYC Taxi website** using **Azure Data Factory (ADF)**.  
âœ… Data stored in **Azure Data Lake Gen2 (Parquet format)**.  
âœ… **Dynamic pipeline** pulls **monthly data** using API.  

### **Phase 2: Data Transformation (Silver Layer)**  
ðŸ“Œ **Azure Databricks & PySpark**: Cleaning & structuring data.  
ðŸ“Œ Secure access using **Azure Service Principal**.  

### **Phase 3: Data Modeling & Serving (Gold Layer)**  
ðŸ“Œ **Delta Lake** ensures **ACID transactions, versioning & time travel**.  
ðŸ“Œ Data is connected to **Power BI** for reporting.  

---

## âš ï¸ Challenges & Solutions  

**âŒ Issue:** Formatting error in month numbers during API calls.  
**ðŸ” Discovery:** The pipeline failed for months **10, 11, 12** due to incorrect URL formatting.  
**âœ… Solution:** Introduced **If Condition** in **Azure Data Factory**:  
âœ” **Months â‰¤ 9** â†’ Keep **leading zero** (e.g., `01.parquet`)  
âœ” **Months > 9** â†’ Remove leading zero (e.g., `10.parquet`)  

This **dynamic logic** ensured smooth API calls for all **12 months**! ðŸš€  

---
